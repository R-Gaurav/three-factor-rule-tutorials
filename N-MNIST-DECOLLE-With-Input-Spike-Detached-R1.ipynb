{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f9d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from decolle_loss import DECOLLELoss\n",
    "import lava.lib.dl.slayer as slayer\n",
    "from nmnist_dataset import NMNISTDataset, augment # Import NMNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ae1df",
   "metadata": {},
   "source": [
    "# Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c26441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DECOLLENetwork(torch.nn.Module):\n",
    "    def __init__(self, input_shape, hidden_shape, output_shape, burn_in=0):\n",
    "        super(DECOLLENetwork, self).__init__()\n",
    "        neuron_params = {\n",
    "            \"threshold\": 1.25,\n",
    "            \"current_decay\": 0.25,\n",
    "            \"voltage_decay\": 0.03,\n",
    "            \"tau_grad\": 0.03,\n",
    "            \"scale_grad\": 3,\n",
    "            \"requires_grad\": True,\n",
    "            \"persistent_state\": True\n",
    "        }\n",
    "\n",
    "        neuron_params_drop = {**neuron_params}\n",
    "        self.burn_in = burn_in\n",
    "        \n",
    "        self.blocks = torch.nn.ModuleList() # Network's feedforward blocks.\n",
    "        self.readout_layers = torch.nn.ModuleList() # Network's DFA weights to calculate loss.\n",
    "        hidden_shape = [input_shape] + hidden_shape\n",
    "        \n",
    "        for i in range(len(hidden_shape)-1):\n",
    "            self.blocks.append(slayer.block.cuba.Dense(\n",
    "            neuron_params_drop, hidden_shape[i], hidden_shape[i+1],\n",
    "            weight_norm=False)\n",
    "            )\n",
    "            \n",
    "            # One fixed readout per layer.\n",
    "            readout = torch.nn.Linear(hidden_shape[i+1], output_shape, bias=False)\n",
    "            readout.weight.requires_grad=False\n",
    "            self.readout_layers.append(readout)\n",
    "            \n",
    "    def forward(self, spike):\n",
    "        spike.requires_grad_() # Set requires grad of input spikes to True.\n",
    "        spikes = []\n",
    "        readouts = []\n",
    "        voltages = []\n",
    "        count = []\n",
    "            \n",
    "        for block in self.blocks:\n",
    "            # Decompose the behaviour of the block to obtain the voltages\n",
    "            # for regularization.\n",
    "            z = block.synapse(spike.detach())\n",
    "            #z = block.synapse(spike)\n",
    "            _, voltage = block.neuron.dynamics(z)\n",
    "            voltages.append(voltage)\n",
    "                \n",
    "            spike = block.neuron.spike(voltage)\n",
    "            spikes.append(spike)\n",
    "            count.append(torch.mean(spike.detach()))\n",
    "            \n",
    "        for ro, spike in zip(self.readout_layers, spikes):\n",
    "            # Compute readouts with layer-wise output spikes as input to the Dense.\n",
    "            readout = []\n",
    "            for t in range(spike.shape[-1]):\n",
    "                readout.append(ro(spike[..., t]))\n",
    "            readouts.append(torch.stack(readout, dim=-1))\n",
    "            \n",
    "        return spikes, readouts, voltages, count\n",
    "        \n",
    "    def init_state(self, inputs, burn_in=None):\n",
    "        self.reset_()\n",
    "        # Initialize the network states + crop inputs.\n",
    "        if burn_in is None:\n",
    "            burn_in = self.burn_in\n",
    "            \n",
    "        self.forward(inputs[..., :burn_in])\n",
    "        return inputs[..., burn_in:]\n",
    "        \n",
    "    def reset_(self):\n",
    "        # reset the states after each example.\n",
    "        for block in self.blocks:\n",
    "            block.neuron.current_state[:] = 0.\n",
    "            block.neuron.voltage_state[:] = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e884a44",
   "metadata": {},
   "source": [
    "# Instantiate Network, Optimizer, Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f285332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NMNIST dataset is freely available here:\n",
      "https://www.garrickorchard.com/datasets/n-mnist\n",
      "(c) Creative Commons:\n",
      "    Orchard, G.; Cohen, G.; Jayawant, A.; and Thakor, N.\n",
      "    \"Converting Static Image Datasets to Spiking Neuromorphic Datasets Using\n",
      "    Saccades\",\n",
      "    Frontiers in Neuroscience, vol.9, no.437, Oct. 2015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_folder = \"Trained\"\n",
    "os.makedirs(trained_folder, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "net = DECOLLENetwork(input_shape=34*34*2,\n",
    "                     hidden_shape=[512, 256],\n",
    "                     output_shape=10,\n",
    "                     burn_in=10).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "training_set = NMNISTDataset(train=True, transform=augment, download=True)\n",
    "testing_set = NMNISTDataset(train=False)\n",
    "\n",
    "train_loader = DataLoader(dataset=training_set, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(dataset=testing_set, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b056109",
   "metadata": {},
   "source": [
    "# Error Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4778f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECOLLELoss is used to compute per-layer pseudo errors.\n",
    "# Regularization allows to control per-layer spike rates.\n",
    "error = DECOLLELoss(torch.nn.CrossEntropyLoss, reg=0.01, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff61f2f8",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a528df75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Epoch: 0, Batch: 100 Done.\n",
      "Training Epoch: 0, Batch: 200 Done.\n",
      "Training Epoch: 0, Batch: 300 Done.\n",
      "Training Epoch: 0, Batch: 400 Done.\n",
      "Training Epoch: 0, Batch: 500 Done.\n",
      "Partial Testing Done. Breaking loop...\n",
      "              \n",
      "Test loss = 360.149810 (min = 360.149810)   accuracy = 0.572500 (max = 0.572500)\n",
      "Training Epoch: 1, Batch: 100 Done.\n",
      "Training Epoch: 1, Batch: 200 Done.\n",
      "Training Epoch: 1, Batch: 300 Done.\n",
      "Training Epoch: 1, Batch: 400 Done.\n",
      "Training Epoch: 1, Batch: 500 Done.\n",
      "Partial Testing Done. Breaking loop...\n",
      "              \n",
      "Test loss = 337.444161 (min = 337.444161)   accuracy = 0.700000 (max = 0.700000)\n",
      "Training Epoch: 2, Batch: 100 Done.\n",
      "Training Epoch: 2, Batch: 200 Done.\n",
      "Training Epoch: 2, Batch: 300 Done.\n",
      "Training Epoch: 2, Batch: 400 Done.\n",
      "Training Epoch: 2, Batch: 500 Done.\n",
      "Partial Testing Done. Breaking loop...\n",
      "              \n",
      "Test loss = 328.739234 (min = 328.739234)   accuracy = 0.747500 (max = 0.747500)\n",
      "Training Epoch: 3, Batch: 100 Done.\n",
      "Training Epoch: 3, Batch: 200 Done.\n",
      "Training Epoch: 3, Batch: 300 Done.\n",
      "Training Epoch: 3, Batch: 400 Done.\n",
      "Training Epoch: 3, Batch: 500 Done.\n",
      "Partial Testing Done. Breaking loop...\n",
      "              \n",
      "Test loss = 321.960141 (min = 321.960141)   accuracy = 0.798750 (max = 0.798750)\n",
      "Training Epoch: 4, Batch: 100 Done.\n",
      "Training Epoch: 4, Batch: 200 Done.\n",
      "Training Epoch: 4, Batch: 300 Done.\n",
      "Training Epoch: 4, Batch: 400 Done.\n",
      "Training Epoch: 4, Batch: 500 Done.\n",
      "Partial Testing Done. Breaking loop...\n",
      "              \n",
      "Test loss = 324.066249 (min = 321.960141)   accuracy = 0.788750 (max = 0.798750)\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "training_mode = \"batch\" # either \"online\" or \"batch\".\n",
    "\n",
    "test_losses = []\n",
    "test_accs_l1 = [] # Test accuracies of the first layer.\n",
    "test_accs_l2 = [] # Test accuracies of the second layer.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_iter = iter(train_loader)\n",
    "    net.train()\n",
    "    batch = 0\n",
    "    for inputs, target in train_iter: # training loop.\n",
    "        batch += 1\n",
    "        inputs = inputs.reshape([inputs.shape[0], -1, inputs.shape[-1]]).to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        # Reset network's state + burn-in + resize the inputs accordingly.\n",
    "        inputs = net.init_state(inputs)\n",
    "        \n",
    "        # training phase.\n",
    "        if training_mode == \"online\":\n",
    "            for t in range(inputs.shape[-1]):\n",
    "                x = inputs[..., t].unsqueeze(-1)\n",
    "                spikes, readouts, voltages, count_t = net(x)\n",
    "                loss = error(readouts, voltages, target)\n",
    "                loss.backward()\n",
    "                \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "        else:\n",
    "            spikes, readouts, voltages, count = net(inputs)\n",
    "            loss = error(readouts, voltages, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        if batch%100==0:\n",
    "            print(\"Training Epoch: %s, Batch: %s Done.\" % (epoch, batch))\n",
    "        if batch == 500:\n",
    "            break\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        test_iter = iter(test_loader)\n",
    "        net.eval()\n",
    "        preds_test_l1 = torch.Tensor()\n",
    "        preds_test_l2 = torch.Tensor()\n",
    "        targets_test = torch.Tensor()\n",
    "        \n",
    "        test_losses.append(0.)\n",
    "        batch = 0\n",
    "        for inputs, target in test_iter: # test loop.\n",
    "            with torch.no_grad():\n",
    "                inputs = inputs.reshape([inputs.shape[0], -1, inputs.shape[-1]]).to(device)\n",
    "                target = target.to(device)\n",
    "                \n",
    "                # Reset net state + burn-in + resize the inputs accordingly.\n",
    "                inputs = net.init_state(inputs)\n",
    "                \n",
    "                # Forward pass + record loss.\n",
    "                spikes, readouts, voltages, count = net(inputs)\n",
    "                loss = error(readouts, voltages, target)\n",
    "                test_losses[epoch] += loss.cpu().numpy()\n",
    "                \n",
    "                preds_test_l1 = torch.cat((preds_test_l1, torch.mean(readouts[0], dim=-1).argmax(-1).cpu()))\n",
    "                preds_test_l2 = torch.cat((preds_test_l2, torch.mean(readouts[1], dim=-1).argmax(-1).cpu()))\n",
    "                targets_test = torch.cat((targets_test, target.cpu()))\n",
    "            batch += 1\n",
    "            if batch == 100:\n",
    "                print(\"Partial Testing Done. Breaking loop...\")\n",
    "                break\n",
    "                \n",
    "        acc_test_l1 = torch.mean((preds_test_l1 == targets_test).type(torch.float))\n",
    "        acc_test_l2 = torch.mean((preds_test_l2 == targets_test).type(torch.float))\n",
    "        \n",
    "        test_accs_l1.append(acc_test_l1.cpu().numpy())\n",
    "        test_accs_l2.append(acc_test_l2.cpu().numpy())\n",
    "        \n",
    "    if (epoch+1)%1 == 0:\n",
    "        print(\"\\r\", \" \"*len(f\"\\r[Epoch {epoch:2d}/{epochs}]\"))\n",
    "        print(\"Test loss = %f (min = %f)   accuracy = %f (max = %f)\" % (\n",
    "              test_losses[epoch], np.min(test_losses), acc_test_l2, np.max(test_accs_l2)))\n",
    "    \n",
    "    if acc_test_l2 >= np.max(test_accs_l2):\n",
    "        torch.save(net.state_dict(), \"./\" + trained_folder + \"/network.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ef1d96",
   "metadata": {},
   "source": [
    "# Investigating DECOLLE Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e15a71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Dense(\n",
       "    (neuron): Neuron()\n",
       "    (synapse): Dense(2312, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "  )\n",
       "  (1): Dense(\n",
       "    (neuron): Neuron()\n",
       "    (synapse): Dense(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96bbb4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): Linear(in_features=512, out_features=10, bias=False)\n",
       "  (1): Linear(in_features=256, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.readout_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45353ebb",
   "metadata": {},
   "source": [
    "## Accuracies from readout Layer 1 and Layer 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c53c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.665, dtype=float32),\n",
       " array(0.71125, dtype=float32),\n",
       " array(0.74, dtype=float32),\n",
       " array(0.7425, dtype=float32),\n",
       " array(0.75625, dtype=float32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accs_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d726bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(0.5725, dtype=float32),\n",
       " array(0.7, dtype=float32),\n",
       " array(0.7475, dtype=float32),\n",
       " array(0.79875, dtype=float32),\n",
       " array(0.78875, dtype=float32)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_accs_l2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
